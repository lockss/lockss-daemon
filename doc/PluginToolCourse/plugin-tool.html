<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <title>LOCKSS: Using the Plugin Generation Tool</title>
  <meta name="GENERATOR" content="Quanta Plus">
</head>
<body>
<h2 style="text-align: center;">Using The LOCKSS Plugin Generation Tool</h2>
<h3 style="text-align: center;">The LOCKSS Team</h3>
<h3 style="text-align: center;">7/21/04</h3>
<h4>Introduction</h4>
Every journal that LOCKSS might preserve has some peculiarities; some
are more peculiar than others. The LOCKSS daemon gets its knowledge of
these peculiarities, be they large or small, from a <span
 style="font-style: italic;">plugin</span>. The plugin for a complex
journal is written mostly in Java, but for most simple journals the
plugin is an XML file which is interpreted by a generic, or <span
 style="font-style: italic;">definable</span>, plugin written by the
LOCKSS team. Here we introduce the LOCKSS plugin generation tool, which
provides a user interface that allows
you to create and test a plugin for most simple journals with no
programming. The user provides input describing the chosen journal, the
tool outputs a definition of the plugin for the chosen journal in XML.
In cases where a single publishing platform, for example HighWire
Press, publishes a large number of journals a single (complex) plugin
will support all of the journals. This document focuses on the simpler
journals, whose plugin is within the capabilities of the plugin
generation tool, so we assume below that there is a single plugin for
each journal.<br>
<br>
Eventually, the LOCKSS daemon will obtain this XML file via a <span
 style="font-style: italic;">plugin registry</span>, a search facility
linking bibliographic information about journals to the appropriate
plugin. Until the plugin registry is working, the XML files defining
plugins must be e-mailed to lockssbeta "at" lockss.org. After testing,
the LOCKSS team will include them in the daemon distribution.<br>
<br>
One major function of the plugin for a journal is to divide the
journal's content into manageable chunks called <span
 style="font-style: italic;">archival units</span> (AUs). Typically, an
AU will consist of a year's run of a journal, or a volume. Among the
information the plugin needs about an AU is its crawl rules. These tell
the LOCKSS web crawler where to stop when it is trying to find newly
published content to collect.&nbsp; Other information includes the
publisher manifest page, which tells the crawler where to start,&nbsp;
and the crawl interval, which tells it how often to start.&nbsp; We use
examples of some real journals to show how you find this information
from the journal's web site, how you feed it into the tool to generate
a suitable plugin, and how you test the plugin to be sure you have the
information correct.<br>
<h4>Obtaining and Running The Tool</h4>
The plugin tool is available in three ways, on CD, by download or via
CVS:<br>
<ul>
  <li>The CD, whose .iso image is available from Sourceforge(<a
 class="moz-txt-link-freetext"
 href="http://prdownloads.sourceforge.net/lockss/plugin-tool-0.6.3.iso?download">plugin-tool-0.6.3.iso</a>, <a class="moz-txt-link-freetext"
 href="http://prdownloads.sourceforge.net/lockss/plugin-tool-0.6.3.iso.md5?download">plugin-tool-0.6.3.iso.md5</a>) and runs on both Windows and Linux/x86:</li>
  <ul>
    <li>On Windows the tool will automatically run when the CD is
inserted.</li>
    <li>On Linux/x86 you should navigate to the linux folder on the CD
and click on the runtool icon.</li>
  </ul>
  <li>You can download two versions: <br>
<b>Unix</b>: <a href="http://prdownloads.sourceforge.net/lockss/plugin-tool-0.6.3.tgz?download">plugin-tool-0.6.3.tgz</a>, 
<a href="http://prdownloads.sourceforge.net/lockss/plugin-tool-0.6.3.tgz.md5?download">plugin-tool-0.6.3.tgz.md5</a>
<br>
<b>Windows</b>: <a href="http://prdownloads.sourceforge.net/lockss/plugin-tool-0.6.3.zip?download">plugin-tool-0.6.3.zip</a>,
<a href="http://prdownloads.sourceforge.net/lockss/plugin-tool-0.6.3.zip.md5?download">plugin-tool-0.6.3.zip.md5</a>
<br>

Download and unpack the appropriate one into a directory (folder) change into that directory and
invoke the script in that directory that runs the tool, <span
 style="font-weight: bold;">runtool</span> for
Unix and <span style="font-weight: bold;">runtool.bat</span> for
Windows.</li>
  <li>You can check out the <span style="font-weight: bold;">lockss-daemon</span>
project from <span style="font-weight: bold;">cvs.sourceforge.net:/cvsroot/lockss</span>
and run the command <span style="font-weight: bold;">ant run-tool
-Dclass=org.lockss.devtools.plugindef.PluginDefinerApp</span>.<br>
  </li>
</ul>
If you have encounter any bugs or have feature requests, please put them in our <a href=http://sourceforge.net/tracker/?group_id=47774>Sourceforge Issue Trackers</a>, selecting "Plug-in Tool" as the category.

<h4>Before You Start Clicking<br>
</h4>
<img src="initial-window.gif" title="" alt="Initial window"
 style="width: 390px; height: 373px;" align="right">When the generator
starts you will see a window which contains the
fields needed to define the
plug-in, like this: <br clear="all">
The
pull-down menus for this window are:<br>
<ul>
  <li>File -&gt; New, Open, Save, Save As, Exit - these allow you to
load and save the XML files defining the plugin.<br>
  </li>
  <li>Edit -&gt; Cut, Copy, Paste, Delete - these operate on editable
text fields.<br>
  </li>
  <li>Plugin -&gt; Expert Mode, Test Crawl Rules ..., Test Filters -
these are described below.</li>
  <li>Help -&gt; About - Brings up version and copyright info.<br>
  </li>
</ul>
Expert Mode allows more control over the plugin, the details are <a
 href="#ExpertMode">here</a>. In most cases you will not need it.
Instead you
will define your plugin by filling in the fields in the window in turn.
They are:<br>
<ul>
  <li>Plugin Name: The human-readable name that the plugin will appear
under in
the
configuration menu of the LOCKSS administrative user interface. Spaces
are OK here.</li>
  <li>Plugin ID:&nbsp; the Java class name for the plugin.<br>
  </li>
  <li>Plugin Version: A version number that differentiates different
versions of the same plugin.&nbsp; It should start at 1.<br>
  </li>
  <li>Configuration Parameters:&nbsp; This defines the set of
parameters that the plugin will use to identify and differentiate
between each AU that uses the plugin.<br>
  </li>
  <li>AU Name Template: This template tells the plugin how to use the
Configuration Parameters to create the name of
each AU using the plugin used by the LOCKSS administrative UI.</li>
  <li>Start URL Template: This template tells the plugin where on the
publisher's web site to find the <span style="font-style: italic;">publisher
manifest page</span> for each AU of the journal.</li>
  <li>Crawl Rules: These rules define the boundaries of an <span
 style="font-style: italic;">archival unit</span> (AU) in the journal's
web site. An AU is normally a year's run or a volume of the journal.<br>
  </li>
  <li>Pause Time Between Fetches: The time for which the LOCKSS daemon
waits after fetching each page from the publisher's web site.<br>
  </li>
  <li>New Content Crawl Interval: The time between attempts by the
LOCKSS daemon to find new content on the publisher's web site.</li>
</ul>
<h4>Example Journal</h4>
We will use the real journal <a href="http://disputatio.com">Disputatio</a>
as an example. Its "home page" is at <a href="http://disputatio.com">http://disputatio.com</a>.
Its publisher manifest page for 2004 is at <a
 href="http://disputatio.com/lockss2004.html">http://disputatio.com/lockss2004.html</a>.
A typical article from 2004 is at <a
 href="http://disputatio.com/articles/016-3.pdf">http://disputatio.com/articles/016-3.pdf</a>
- it is the third article in issue number 16 of the journal. That's all
the information we need to get started.<br>
<br>
<li>A more detailed discussion of the information you might need is in
the document "<a
 href="https://sourceforge.net/docman/display_doc.php?docid=20560&amp;group_id=47774"
 name="Writing A Simple Plug-in" type="text/html">Writing a Simple
Plug-in</a>". <br>
</li>
<h4>Defining a basic plug-in</h4>
<ul>
  <li>We will call the Disputatio plugin "Disputatio". Type that into
the "Plugin Name" field.</li>
  <li>The Java name for the plugin will be the reverse of your
institution's DNS domain followed plugin followed by a class name. For
example, ours will be called <span style="font-weight: bold;">org.lockss.plugin.DisputatioPlugin</span>.
Click on the NONE and type it, substituting your DNS name for <span
 style="font-weight: bold;">org.lockss</span>.<br>
  </li>
  <li>We are defining version 1 of the Disputatio plugin, so make sure
the version field is 1.</li>
  <li><img src="configuration.gif" title=""
 alt="Default configuration window" style="width: 490px; height: 282px;"
 align="right">Now click on the row of dots beside "Configuration
Parameters". A
window pops up looking like this: In this case, the only
parameters we need the
administrator to define are the <span style="font-weight: bold;">base_url</span>,
which is
always required so that the plugin can find the journal (it will be <span
 style="font-weight: bold;">http://disputatio.com/</span>), and the <span
 style="font-weight: bold;">year</span>
(it will be <span style="font-weight: bold;">2004</span>, etc.), so it
can find the
publisher
manifest pages (at <span style="font-weight: bold;">http//disputatio.com/lockss2004.html</span>,
etc.). Select <span style="font-weight: bold;">year</span> and click
Add.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="configuration2.gif" title=""
 alt="Configuration window filled in"
 style="width: 490px; height: 282px;" align="right">The window should
look like this: Now click OK.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="url-template.gif" title=""
 alt="URL template initial state" style="width: 395px; height: 374px;"
 align="right">Now we use these
parameters to define the Start URL Template, which
tells the
plugin where to find the publisher manifest page. Click on the NONE
beside it. A template editor
window pops up, looking like this:<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="url-template2.gif" title=""
 alt="URL Name template filled in" style="width: 395px; height: 374px;"
 align="right">Choose "Base URL" from the pull-down menu and click
"Insert Parameter". Choose "String Literal" and click "Insert
Parameter"; type <span style="font-weight: bold;">lockss</span> into
the popup then click "OK". Choose "year" from the pull-down menu and
click "Insert Parameter".&nbsp; Choose "String Literal" and click
"Insert Parameter"; type <span style="font-weight: bold;">.html</span>
into the popup then click "OK". The "Editor View" tab shows you the
resulting format string into which the
parameters in blue are
substituted to obtain, in our case, <span style="font-weight: bold;">http://disputatio.com/lockss2004.html</span>
etc.&nbsp;
Click Save.<br clear="all">
  </li>
</ul>
<br>
<ul>
</ul>
<ul>
  <li><img src="au-name.gif" title=""
 alt="AU Name template initial state"
 style="width: 395px; height: 374px;" align="right">Next we specify the
name that each AU
captured
by this plugin will have in the LOCKSS Administrative user interface.
Click on the NONE beside AU Name Template to get a template editor
window that looks like this:<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="au-name2.gif" title="" alt="AU Name Template filled in"
 style="width: 395px; height: 374px;" align="right">We want
the name of the 2004 AU to be <span style="font-weight: bold;">http://disputatio.com
2004</span>, i.e. the <span style="font-weight: bold;">base_url</span>
followed by a space
followed by the <span style="font-weight: bold;">year</span>.
Choose "Base URL" from the pull-down menu, click "Insert Parameter".
Choose "String Literal" from the pull-down menu, click "Insert
Parameter", type a space in the popup then click "OK". Choose <span
 style="font-weight: bold;">year</span> from the pull-down menu, click
"Insert Parameter" and
the
window looks like this:&nbsp;
The <span style="font-weight: bold;">base_url</span> parameter is
substituted
for the %s and the <span style="font-weight: bold;">year</span> for
the %d to achieve what we want. Click Save.<br clear="all">
    <br>
  </li>
</ul>
<ul>
  <li>Next we need to specify a series of crawl rules that the crawler
applies
in order to each URL, starting with the publisher manifest page's URL
and continuing with the URLs found in it, and the URLs found in those
pages, and so on. Click on the row of dots beside to get a window that
looks
like this:<img src="crawl-template.gif" title=""
 alt="Crawl rule template initial state"
 style="width: 450px; height: 132px;" align="right">&nbsp; The left
column contains actions that are taken if the URL matches the pattern
in the right column (see Roundup issue 993). Two rules are
pre-specified for
you; they will be needed in almost every case. The first says that
anything that doesn't start with base_url (in our case, <a
 href="http://disputatio.com/">http://disputatio.com/</a>) will
be excluded. The second says that the
publisher manifest page will be included. The rules are in the form of
a printf format string which has the configuration parameters for the
AU
substituted into it before being used as a regular expression pattern.
Regular expressions are full of pitfalls for the unwary. They use many
meta characters with special meanings (the ^ at the beginning of the
first rule is an example, it forces the pattern to be matched only if
it starts at the beginning of the URL). So the combination of format
string and regular expression would be hard to type by hand. The tool
makes this easy by building them up in stages. If you really want the
gory details, look <a
 href="http://directory.google.com/Top/Computers/Programming/Languages/Regular_Expressions/FAQs,_Help,_and_Tutorials/">here</a>.<br>
  </li>
  <li>&nbsp;Now we
want to make sure that any articles linked from the manifest page are
included. A
typical URL we want to include is <a
 href="http://disputatio.com/articles/016-3.pdf">http://disputatio.com/articles/016-3.pdf</a>
- so we want the pattern to match the <span style="font-weight: bold;">base_url</span>
followed by the text <span style="font-weight: bold;">articles/</span>
followed by anything followed by <span style="font-weight: bold;">.pdf</span>.</li>
  <li><img src="crawl-template2.gif" title=""
 alt="Pattern editor initial state" style="width: 395px; height: 374px;"
 align="right">Click Add to get a default rule to edit. It will have
the
Include action, which is what we want, and the NONE pattern, which we
edit by clicking on it to get a window that looks like this:<br
 clear="all">
  </li>
</ul>
<ul>
  <li><img src="crawl-template3.gif" title="" alt="Crawl rule stage 1"
 style="width: 395px; height: 374px;" align="right">Choose "Base URL"
from the "Insert Parameter" pull-down menu and click on "Insert
Parameter" to
start the pattern with <span style="font-weight: bold;">base_url</span>,
like this:<br clear="all">
  </li>
</ul>
<br>
<br>
<ul>
  <li><img src="crawl-template4.gif" title=""
 alt="Pattern editor stage 2" style="width: 395px; height: 374px;"
 align="right">Now pull down the menu by
"Insert Match" and choose "String Literal". Click "Insert Match" and a
box pops up into which you type the text you want to match, in the case
    <span style="font-weight: bold;">articles/</span>. Click OK and the
pattern editor window looks like this:<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="crawl-template5.gif" title=""
 alt="Pattern editor stage 4" style="width: 395px; height: 374px;"
 align="right">Next we want to match anything, so pull down the menu by
"Insert
Match", choose&nbsp; "Anything" and click "Insert Match". The pattern
editor window looks like:&nbsp;
Note the .* you just inserted. This is a regular expression made of two
meta characters. The period matches any single character, the star
means
any sequence of characters matching the previous character which,
because it is a period, matches anything. So .* matches any string of
characters, specifically in our case strings like <span
 style="font-weight: bold;">016-3</span>.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="crawl-template6.gif" title=""
 alt="Pattern editor stage 5" style="width: 395px; height: 374px;"
 align="right">Next we want to match <span style="font-weight: bold;">.pdf</span>.
Pull down the menu by "Insert Match", choose "String Literal" and click
Insert Match. A box pops up into which you type <span
 style="font-weight: bold;">.pdf</span>. Click OK. The pattern editor
window now looks like:&nbsp;
Note the backslash (\) before the period before <span
 style="font-weight: bold;">pdf</span>. Period is a regular expression
meta character, so it must be escaped with a backslash beforehand if it
is to match an actual period. The tool generates these "escape"
meta characters automatically.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="crawl-template7.gif" title=""
 alt="Crawl rule window with third rule"
 style="width: 450px; height: 232px;" align="right">Now we're done with
this rule, so click Save. The Crawl Rule
window now looks like this: <br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="base-window2.gif" title="" alt="Base window after edits"
 style="width: 390px; height: 373px;" align="right">We're done for now,
so click OK. The base plugin editor
window
now looks like:<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="test-crawl.gif" title=""
 alt="First stage of crawl rule test"
 style="width: 410px; height: 160px;" align="right">We are now ready to
start testing the plugin.&nbsp; Pull
down the
"Plugin" menu and choose "Test Crawl Rules ...". A window pops up that
allows you to provide the configuration parameters you decided earlier
that were needed to identify an AU. Type
values for the configuration parameters, in our case <span
 style="font-weight: bold;">base_url</span> http://disputatio.com/ and <span
 style="font-weight: bold;">year</span> 2004. Note the trailing / on
the <span style="font-weight: bold;">base_url</span>. Click "Check AU".<br
 clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="test-crawl2.gif" title="" alt="Before crawl rule test"
 style="width: 410px; height: 432px;" align="right">Now a window
looking like this pops up with pre-loaded fields for the publisher
manifest page you
want to start testing from, in our case
http://disputation.com/lockss2004.html, the Test Depth (a file
linked from the manifest page has a depth of 1, a page linked from that
page has a depth of 2, and so on) and Fetch Delay (the number of
seconds to pause between fetching pages): <br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="test-crawl3.gif" title="" alt="Results of test"
 style="width: 410px; height: 432px;" align="right">Click "Check URL"
to start the test. The test results appear in the scrolling text pane:<br
 clear="all">
  </li>
</ul>
<br>
<ul>
  <li>In our case the results pane ends
up containing <a href="disputatio1.txt">this</a>. <img
 src="crawl-template8.gif" title="" alt="Final crawl rules"
 style="width: 450px; height: 217px;" align="right">You will see that
our rules correctly included all the
articles
and correctly excluded http://lockss.stanford.edu/, but there were some
files excluded that should have been included. We need to add rules
including <span style="font-weight: bold;">base_url</span> followed by
anything followed by <span style="font-weight: bold;">.js</span>, <span
 style="font-weight: bold;">base_url</span> followed by anything
followed by <span style="font-weight: bold;">.css</span> and <span
 style="font-weight: bold;">base_url</span> followed by <span
 style="font-weight: bold;">images/</span> followed by anything
followed by <span style="font-weight: bold;">.gif</span>. Click on the
row of dots by "Crawl Rules" and follow the same process you used to
insert the <span style="font-weight: bold;">articles/</span> rule to
insert these rules. The Crawl Rule window should look like: <br
 clear="all">
  </li>
</ul>
<br>
<ul>
  <li>Click OK and test this again. The results pane should contain <a
 href="disputatio2.txt">this</a>. We have correctly included the
articles and the other files.</li>
  <li>Disputatio publishes two issues per year, so checking every three
weeks is excessive. Set the New Content Crawl Interval to 13 weeks.<br>
  </li>
  <li>Finally, save your plugin as an XML file using Save or Save
As.&nbsp; Submit the
resulting file to the LOCKSS team for testing and inclusion
in the distribution.</li>
</ul>
<h4><a name="Improving_the_Design_of_the_Example"></a>Improving the
Design of the Example Plugin</h4>
Although the plugin we just defined includes and excludes the correct
files, it does so by explicitly specifying the files on the publisher's
site to include. In most cases it will be better to explicitly specify
the files to exclude, and include everything else. This approach is
more likely to work as the publisher tweaks their site.<br>
Which files on the Disputatio site do we know should be excluded? The
only ones we are sure should be excluded are the "home page" (we know
this will change with every new issue) and the publisher manifest pages
for other AUs (e.g the 2004 AU should not include the 2003 publisher
manifest page). A better plugin design would have rules that:<br>
<ul>
  <li>Exclude everything not starting with the <span
 style="font-weight: bold;">base_url</span> <a
 href="http://disputatio.com/">http://disputatio.com/</a> (the first
default rule).</li>
  <li>Include the publisher manifest page for this AU (the second
default rule).</li>
  <li>Exclude the publisher manifest pages from other AUs, in our case
everything matching the <span style="font-weight: bold;">base_url</span>
followed by the string literal <span style="font-weight: bold;">lockss</span>
followed by a number followed by <span style="font-weight: bold;">.html</span>.
Although this will match the publisher manifest page for this AU, that
page has already been included so this exclusion rule will not affect
it.<br>
  </li>
  <li>Exclude the "home page" under both its aliases. First as <a
 href="http://disputatio.com/">http://disputatio.com/</a>, i.e. a
pattern starting with <span style="font-weight: bold;">base_url</span>
followed by the end of the string. Second as <a
 href="http://disputatio.com/index.html">http://disputatio.com/index.html</a>,
i.e. a pattern starting with <span style="font-weight: bold;">base_url</span>
followed by the string literal <span style="font-weight: bold;">index.html</span>.</li>
</ul>
Restart the plugin tool and fill in the fields as before until you get
to the Crawl Rules. As before, when you click on the row of dots by
Crawl Rules, the window that pops up will have the first two rules
pre-defined. Now add the extra exclusion rules outlined above:<br>
<ul>
  <li>Exclude the publisher manifest pages for other AUs by clicking
Add, pulling down the menu from Include and choosing Exclude. Click on
NONE and choose:</li>
  <ul>
    <li>Insert Match, Start</li>
    <li>Insert Match, String Literal, lockss</li>
    <li>Insert Match, Any Number</li>
    <li>Insert Match, String Literal, .html</li>
    <li>Save</li>
  </ul>
  <li>Exclude the home page under its http://disputatio.com/ alias by
clicking Add, pulling down the menu from Include and choosing Exclude.
Click on NONE and choose:</li>
  <ul>
    <li>Insert Match, Start</li>
    <li>Insert Parameter, base_url</li>
    <li>Insert Match, End</li>
    <li>Save</li>
  </ul>
  <li>Exclude the home page under its http://disputatio.com/index.html
alias by clicking Add, pulling
down the menu from Include and choosing Exclude. Click on NONE and
choose:</li>
  <ul>
    <li>Insert Match, Start</li>
    <li>Insert Parameter, base_url</li>
    <li>Insert Match, String Literal, index.html</li>
    <li>Insert Match, End</li>
  </ul>
</ul>
<br>
<ul>
  <li><img src="crawl-template9.gif" title=""
 alt="Alternate crawl rules" style="width: 450px; height: 217px;"
 align="right">The Crawl Rule
window will look like this:&nbsp; Note
the backslashes before the periods in the .html string literals in the
patterns - the tool automatically inserts these to prevent the period
being interpreted as a regular expression meta character.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li>The result of testing this set of rules is <a
 href="disputatio3.txt">this</a>. Only the publisher manifest page is
included, because the default
action if no rules are matched is to exclude the URL. We need to add a
rule to include everything that isn't being explicitly excluded. Click
Add. Click on NONE and
choose:</li>
  <ul>
    <li>Insert Match, Start</li>
    <li>Insert Parameter, base_url</li>
    <li>Insert Match, Anything</li>
  </ul>
</ul>
<br>
<ul>
  <li><img src="crawl-template10.gif" title=""
 alt="Improved crawl rules" style="width: 450px; height: 217px;"
 align="right">Now the Crawl Rule
window will look like this:&nbsp; You
may need to expand the window to see it all (see Roundup issue 990).
The result of testing this set
of crawl rules is <a href="disputatio4.txt">this</a>, which is
correct, and less likely than the original example to break as
the publisher changes their site.<br clear="all">
  </li>
</ul>
&nbsp;<br>
<h4>A More Complex Example Journal</h4>
We will use <a href="http://www.shu.ac.uk/emls/emlshome.html">Early
Modern Literary Studies</a> (EMLS) as an example of a somewhat more
complex journal and thus a somewhat more complex plugin. Its home page
and the <span style="font-weight: bold;">base_url</span> is at <a
 href="http://www.shu.ac.uk/emls/">http://www.shu.ac.uk/emls/</a>. The
publisher manifest page for Volume 9 is at <a
 href="http://www.shu.ac.uk/emls/lockss-volume9.html">http://www.shu.ac.uk/emls/lockss-volume9.html</a>.
EMLS normally publishes every 4 months but it also publishes "special
editions" on an irregular schedule. Each volume has a table of contents
page, for example <a href="http://www.shu.ac.uk/emls/09-3/09-3toc.htm">http://www.shu.ac.uk/emls/09-3/09-3toc.htm</a>.
The articles in the volume are at URLs like <a
 href="http://www.shu.ac.uk/emls/09-3/finntabl.htm">http://www.shu.ac.uk/emls/09-3/finntabl.htm</a>,
i.e. the directory (folder) emls/09-3 contains the pages for volume 9
number 3. We will pretend that this plugin is being developed at
Sheffield Hallam University (<a href="http://www.shu.ac.uk">http://www.shu.ac.uk</a>)<br>
<br>
<ul>
  <li>Start the tool. Set the Plugin Name to <span
 style="font-weight: bold;">Early Modern Literary Studies</span> and
the Plugin ID to <span style="font-weight: bold;">uk.ac.shu.plugin.EMLSPlugin</span>.</li>
  <li>Pop up the Configuration Parameters dialog. Add volume to the set
of configuration parameters, we need it because it is part of the
publisher manifest page URL.</li>
  <li><img src="emls-url.gif" title="" alt="EMLS Start URL dialog"
 style="width: 395px; height: 374px;" align="right">Pop up the Start
URL dialog. Insert <span style="font-weight: bold;">base_url</span>,
type <span style="font-weight: bold;">lockss-volume</span>, insert <span
 style="font-weight: bold;">volume</span>, type <span
 style="font-weight: bold;">.html</span>.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-pad.gif" title="" alt="Numeric padding selection"
 style="width: 400px; height: 227px;" align="right">When you insert <span
 style="font-weight: bold;">volume</span> you will be asked to set the
padding. In this case we need "do not pad" to get, in our case, just
the 9.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-au-name.gif" title="" alt="EMLS AU name"
 style="width: 395px; height: 374px;" align="right">Pop up the AU Name
Template dialog. Insert base_url, type space <span
 style="font-weight: bold;">vol</span> space, insert volume and set the
padding to "no padding".&nbsp; The AU name will look like <span
 style="font-weight: bold;">http://www.shu.ac.uk/emls/ vol 9</span>.<br
 clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-crawl1.gif" title="" alt="EMLS initial crawl rule"
 style="width: 450px; height: 164px;" align="right">Pop up the Crawl
Rules dialog. Include everything that matches Start followed by
base_url followed by volume padded with zeros to a field width of 2
followed by a literal string of <span style="font-weight: bold;">-</span>
followed by Any Number followed by a string literal of <span
 style="font-weight: bold;">/</span> followed by Anything followed by
End.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-crawl2.gif" title="" alt="First EMLS crawl test"
 style="width: 410px; height: 160px;" align="right">Now test this crawl
rule. Configure the AU with the base_url <a
 href="http://www.shu.ac.uk/emls/">http://www.shu.ac.uk/emls/</a> and
the volume 9. Start with the default depth of 1.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li>The results of this test are <a href="emls1.txt">here</a>. Note
that the rules have included the table of contents pages for
the three normal issues of Volume 9, but not the table of contents page
for the special issue 13. They have also excluded some decorations that
should be collected. This test did not collect any articles, but we're
not yet sure whether the rules are to blame. The articles are linked
from a page linked from the publisher manifest page, so they will only
be collected at depth 2 or more.</li>
</ul>
<ul>
  <li><img src="emls-crawl4.gif" title=""
 alt="Including the special edition"
 style="width: 450px; height: 188px;" align="right">Pop up the Crawl
Rules dialog and add a rule that includes the special issue table of
contents for the special issue by including anything that matches Start
followed by <span style="font-weight: bold;">base_url</span> followed
by the string literal <span style="font-weight: bold;">si-</span>
followed by Any Number followed by the string literal <span
 style="font-weight: bold;">/</span> followed by Anything followed by
End.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-crawl3.gif" title="" alt="EMLS crawl rule test"
 style="width: 410px; height: 432px;" align="right">Now test these
crawl rules. Configure the AU with the base_url <a
 href="http://www.shu.ac.uk/emls/">http://www.shu.ac.uk/emls/</a> and
the volume 9. Set the depth to 2. The test will take a while as it
waits the 6 seconds between fetching each page. The results are <a
 href="emls2.txt">here</a>.<br clear="all">
  </li>
</ul>
<br>
<br>
<ul>
  <li><img src="emls-crawl6.gif" title="" alt="Addition rule for .gif"
 style="width: 450px; height: 201px;" align="right">Note that we are
still excluding some .gif files that should be included. Pop up the
Crawl Rules dialog and add a rule that matches <span
 style="font-weight: bold;">base_url</span> followed by Anything
followed by the string literal <span style="font-weight: bold;">.gif</span>
followed by End,<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-crawl5.gif" title=""
 alt="Third test of EMLS crawl rules"
 style="width: 410px; height: 432px;" align="right">Now test these
crawl rules again. Configure the AU with the base_url <a
 href="http://www.shu.ac.uk/emls/">http://www.shu.ac.uk/emls/</a>
and the volume 9. Set the depth to 3. The test will take 5 minutes or
so as it
waits the 6 seconds between fetching each page. The results are <a
 href="emls3.txt">here</a>.<br clear="all">
  </li>
</ul>
<br>
Reviewing this rather long list, it appears that the crawl rules are
now including and excluding the correct files. Note that among the
files included are programs for Windows and the Mac, .gif and .jpg
images and .wrl files.<br>
<h4>Evaluating your Journal</h4>
Now you are ready to evaluate your chosen journal and collect the
information you need to create a plugin for it.&nbsp; In this section
you will collect the information, in following sections we cover some
of the issues you may run in to that we haven't covered so far. Point
your browser at your journal's home page and start answering the
questions below:<br>
<br>
<ol>
  <li>What is the URL of the journal's home page?</li>
  <li>Is the journal structured as years, volumes, a sequence of
issues, or some other way?</li>
  <li>If there is a volume table of contents page:</li>
  <ol>
    <li>What is the URL for the volume before the current one?</li>
    <li>Does this page link to all other volumes, to the next and
previous volumes, or to no other volumes?</li>
    <li>Chose a typical issue in this volume.<br>
    </li>
  </ol>
  <li>If there is a year table of contents page:</li>
  <ol>
    <li>What is the URL for the year 2003?</li>
    <li>Does this page link to all other years, to the next and
previous years, or to no other years?<br>
    </li>
    <li>Choose a typical issue from 2003.</li>
  </ol>
  <li>Is there a table of contents page for your chosen issue? If so:</li>
  <ol>
    <li>What is its URL?</li>
    <li>Does this page link to all other issues, all other issues in
this volume or year, the next and previous issues, or to no other
issues?<br>
    </li>
  </ol>
  <li>Is there a publisher manifest page? If there is, what is its URL?
You will not be able to finish specifying a plugin for the journal or
test the plugin you have specified until the publisher manifest page is
in place.<br>
  </li>
  <li>Choose a typical article in your chosen issue. What is its URL?</li>
  <li>Check the format in which the article is delivered (e.g. PDF,
HTML, ...).</li>
  <li>If the article format is HTML does it contain:</li>
  <ol>
    <li>Advertisements? If so, what is the URL for a typical
advertisement?<br>
    </li>
    <li>Personalizations? If so, what do they look like?<br>
    </li>
    <li>Images? If so, what is the URL for a typical image? (NB sites
often use images for mathematical and other non-standard characters).<br>
    </li>
    <li>Links to cited articles in the same journal? If so, what is the
URL for a typical intra-journal citation?</li>
    <li>Links to cited articles in other journals? If so, what is the
URL for a typical inter-journal citation?</li>
    <li>Links to sound clips? If so:</li>
    <ol>
      <li>What is the URL for a typical sound clip?</li>
      <li>Is the sound streamed or downloaded?</li>
      <li>What format is used?<br>
      </li>
    </ol>
    <li>Links to movies? If so:</li>
    <ol>
      <li>What is the URL for a typical movie?</li>
      <li>Is the movie streamed or downloaded?</li>
      <li>What format is used?<br>
      </li>
    </ol>
    <li>Javascript? If so, what does the Javascript implement (e.g
search button).</li>
    <li>Links to all other articles in the same volume, year or issue,
the next and previous articles, or to no other articles (except for
citations).</li>
    <li>Hit refresh. Identify any elements on the page that have
changed.<br>
    </li>
  </ol>
  <li>Review your answers above. If you have pages that link to all
other similar pages rather than simply next and previous similar pages,
or if you have advertisements, personalizations or page elements that
changed on refresh the plugin for your journal is beyond the current
capabilities of the plugin tool (see <a href="#Limitations">below</a>).
Please discuss your findings with the LOCKSS team. Otherwise you can
proceed to analyze your journal's URL structure.<br>
  </li>
</ol>
<h4>Article-ID Journal Structure</h4>
Some journals encode the volume and issue in their URLs for articles.
An example we used above is EMLS, whose article URLs are like <a
 href="http://www.shu.ac.uk/emls/09-3/finntabl.htm">http://www.shu.ac.uk/emls/09-3/finntabl.htm</a>,
which includes the volume (09) and issue(3).&nbsp; In this case the
crawl rules should&nbsp; include everything that matches Start followed
by
<span style="font-weight: bold;">base_url</span> followed by volume
padded with zeros to a field width of 2
followed by a literal string of <span style="font-weight: bold;">-</span>
followed by Any Number followed by a string literal of <span
 style="font-weight: bold;">/</span> followed by Anything followed by
End.<br>
Some journals encode the issue but not the year or volume in their URLs
for articles. An example we used above is Disputatio, whose article
URLs are like <a href="http://disputatio.com/articles/016-3.pdf">http://disputatio.com/articles/016-3.pdf</a>
- it is the third article in issue number 16 of the journal. In this
case we want to use the exclusionary crawl rules described <a
 href="#Improving_the_Design_of_the_Example">above</a>.<br>
Some journals use parameters on their URLs for articles. An example is
Studies in Nolinear Dynamics and Econometrics, hosted on BePress. A
typical article has a URL like <a
 href="http://www.bepress.com/cgi/viewcontent.cgi?article=1208&amp;context=snde">http://www.bepress.com/cgi/viewcontent.cgi?article=1208&amp;context=snde</a>.&nbsp;
The parameters are the part after the <span style="font-weight: bold;">?</span>,
in this case the <span style="font-weight: bold;">context=</span>
parameter selects the journal among all the journals hosted on BePress
and the <span style="font-weight: bold;">article=</span> parameter
selects the individual article from the journal.&nbsp; In this case we
want to exclude everything that doesn't match <span
 style="font-weight: bold;">base_url</span> (<span
 style="font-weight: bold;">http://www.bepress.com/</span>) followed by
<span style="font-weight: bold;">cgi/viewcontent.cgi?</span> followed
by anything followed by <span style="font-weight: bold;">context=snde</span>.<br>
In some cases of a publisher platform that hosts multiple journals, it
may not be possible to tell from the article URLs whether the article
is part of the journal of interest or not. If this is the case your
journal is beyond the current capabilities of the plugin tool (see
below). Please discuss your findings with the LOCKSS team.<br>
<h4>Archival Unit Design</h4>
So far, we've assumed that the publisher manifest file is a given. In
practice, its typically the result of a negotiation between the library
responsible for selecting a journal and generating the plugin, and the
publisher. The questions that arise are two-fold:<br>
<ul>
  <li>How to divide up the publisher's content into Archival Units that
are a manageable size, and which don't last too long? As a rule of
thumb, if the publisher divides the journal into volumes, each volume
should be an AU. Otherwise, each year of the journal should be an AU.</li>
  <li>What to put in the publisher manifest page, and wher to put it?
As a rule of thumb, if each issue of the journal has a table of
contents page, then the publisher manifest page should point to the
table of contents page for each issue that makes up the AU (volume or
year). Otherwise, if there is a single table of contents page for all
issues in a volume or year pointing directly to their contents, the
manifest page should do the same.</li>
</ul>
Many journals are published using a sophisticated platform, such as <a
 href="http://www.highwire.org/lists/allsites.dtl?view=by+publisher">HighWire
Press</a>, Atypon, the <a
 href="http://www.pkp.ubc.ca/ojs/journals.html">Open Journal System</a>
or <a href="http://muse.jhu.edu/journals/">Project Muse</a>.&nbsp; In
these cases a single plugin will generally work for all journals hosted
on the publishing platform. To do this, the configuration parameters
must be sufficient to distinguish between the AUs of the multiple
journals on the platform. There are two common cases:<br>
<ul>
  <li>Each journal hosted on HighWire Press has its own domain name and
thus its own <span style="font-weight: bold;">base_url</span>. For
example, a typical article in the <span style="font-style: italic;">British
Medical Journal</span> has a URL like <a
 href="http://bmj.bmjjournals.com/cgi/content/full/328/7453/1405">http://bmj.bmjjournals.com/cgi/content/full/328/7453/1405</a>
in which the base_url is <span style="font-weight: bold;">http://bmj.bmjjournals.com/</span>.
A typical article in <span style="font-style: italic;">Proceedings of
the National Academy of Sciences</span> has a URL like <a
 href="http://www.pnas.org/cgi/content/full/97/12/6267">http://www.pnas.org/cgi/content/full/97/12/6267</a>
in which the base_url is <span style="font-weight: bold;">http://www.pnas.org/</span>.
Both journals are hosted in HighWire Press,&nbsp; as can be seen from
the remainder of the URLs, which have the structure <span
 style="font-weight: bold;">cgi/content/full/&lt;volume&gt;/&lt;issue&gt;/&lt;article-id&gt;</span>.
In this case the confguration parameters needed would be <span
 style="font-weight: bold;">base_url</span> and <span
 style="font-weight: bold;">volume</span>.<br>
  </li>
  <li>Each journal hosted on Project Muse has a subdirectory of the
single Project Muse site.&nbsp; For example, a typical article in the <span
 style="font-style: italic;">American Journal of Mathematics</span> has
a URL like <a
 href="http://muse.jhu.edu/journals/american_journal_of_mathematics/v119/119.3li.pdf">http://muse.jhu.edu/journals/american_journal_of_mathematics/v119/119.3li.pdf</a>
in which the <span style="font-weight: bold;">base_url</span> is <span
 style="font-weight: bold;">http://muse.jhu.edu/journals/</span> and
the <span style="font-weight: bold;">journal_dir</span> is <span
 style="font-weight: bold;">american_journal_of_mathematics/</span>. A
typical article in <span style="font-style: italic;">Postmodern Culture</span>
has a URL like <a
 href="http://muse.jhu.edu/journals/pmc/v010/10.3chan.html">http://muse.jhu.edu/journals/pmc/v010/10.3chan.html</a>
in which the <span style="font-weight: bold;">base_url</span> is <span
 style="font-weight: bold;">http://muse.jhu.edu/journals/</span> and
the <span style="font-weight: bold;">journal_dir</span> is <span
 style="font-weight: bold;">pmc/</span>. Both are hosted on Project
Muse, as can be seen from the shared <span style="font-weight: bold;">base_url</span>
and the remainder of the URLs, which start with <span
 style="font-weight: bold;">v&lt;volume&gt;/&lt;volume&gt;.&lt;issue&gt;</span>.
Ideally, the configuration parameters needed should be <span
 style="font-weight: bold;">journal_dir</span> and <span
 style="font-weight: bold;">volume</span>, but for internal reasons the
daemon requires <span style="font-weight: bold;">base_url</span> to be
a configuration parameter in all cases.<br>
  </li>
</ul>
<h4>Crawl Rule Design</h4>
In the examples above we have seen two different approaches to
designing crawl rules; <span style="font-style: italic;">inclusionary</span>
rules which primarily list the URLs that should be fetched and exclude
everything else, and <span style="font-style: italic;">exclusionary</span>
rules that primarily list patterns for URLs that should not be fetched
and fetch everything else. Which approach should you choose?<br>
<ul>
  <li>If the journal has a deep URL structure with a separate directory
for each volume or year, write rules that exclude everything not under
the appropriate volume or year directory. For example, if the URL looks
like <a
 href="http://bmj.bmjjournals.com/cgi/content/full/328/7453/1405">http://bmj.bmjjournals.com/cgi/content/full/328/7453/1405</a>
then excluding everything that doesn't match&nbsp; <span
 style="font-weight: bold;">base_url</span> followed by <span
 style="font-weight: bold;">cgi/content/full</span> followed by <span
 style="font-weight: bold;">volume</span> is a good approach.</li>
  <li>If the journal uses parameters in the URL and they include the
volume or year, write rules that exclude everything that doesn't have
the appropriate parameters.</li>
  <li>If the journal has a flat structure, with everything in a single
directory, write rules that include the files that are actually needed.
For example, if the URL looks like <a
 href="http://disputatio.com/articles/016-3.pdf">http://disputatio.com/articles/016-3.pdf</a>
then including everything that matches <span style="font-weight: bold;">base_url</span>
followed by <span style="font-weight: bold;">articles/</span> followed
by a number followed by <span style="font-weight: bold;">-</span>
followed by a number followed by anything is a good approach.</li>
  <li>If the journal uses parameters in the URL and they don't include
the volume or year, write rules that include the files that are
actually needed.</li>
</ul>
<h4><a name="Limitations"></a>What are the plugin generator's limits?</h4>
The plugin generation tool has limitations; the plugins for many more
complex journals will exceed them. In general, any plugin that requires
knowledge of the Java classes in the daemon is likely to fall into this
class. Although the "Expert Mode" described in the next section allows
plugins to use some pre-defined Java classes, we aren't yet ready to
explain how you find out what these classes are or what exactly they do
for you. We are still working to expand the capabilities of these
pre-defined classes as we build and test plugins for complex journals;
anyone else attempting a complex journal is also likely at present to
find their capabilities inadequate and need to add to them by writing
Java.<br>
<br>
There are a number of warning signs that a journal is complex enough to
be beyond the capabilities of the tool. If you find any of these
features in your chosen journal you should consult the LOCKSS team:<br>
<ul>
  <li>Some journals add advertisements, personalizations and other
dynamic content to the otherwise static pages. The LOCKSS daemon must <span
 style="font-style: italic;">filter</span> the pages of these journals
to remove the additions before comparing them with the same pages at
other caches, which will have received different advertisements, etc.
Filtering requires the use of special filter classes and the tool
support for these is incomplete.<br>
  </li>
  <li>Some journals experience massive spikes of reader interest
immediately a new issue is published. They typically want LOCKSS not to
crawl during these predictable periods of high load, and the LOCKSS
daemon has classes that provide suitable <span
 style="font-style: italic;">crawl windows</span> in time.</li>
  <li>Some journals do not return normal HTTP error codes, such as 404
for "Page not found" but instead return "helpful" pages with a
different return code. The LOCKSS daemon has classes that recognize
these error pages and re-map them to be conventional errors.</li>
  <li>Some journals have sophisticated access control methods or
crawler traps to prevent theft of their valuable content.</li>
  <li>Some journals have media types for which LOCKSS support is not
yet available, for example streaming media. In some cases, such as Real
Audio, LOCKSS does support non-streamed versions but this again is
beyond the tool's current capability.</li>
</ul>
<h4><a name="ExpertMode"></a>Using Expert Mode</h4>
Selecting expert mode in the Plug-in Menu will bring up the remaining
parameters:
<ul>
  <li>Default Crawl Depth: When doing a new content crawl, how many
levels down should it go to check for changes.
The default is 1. It will only check the manifest page for new content.</li>
  <li>Crawl Window Class: The name of the class which encapsulates the
temporal crawl restrictions for the
publishers site.</li>
  <li>Filter Classes: The list of filters by mime-type for filtering
content before performing a hash.</li>
  <li>Crawl Exception Class: The name of the class to be called when
HTTP
errors are generated. This only
needs to be implemented if the site uses HTTP return codes in
non-standard ways.</li>
  <li>Cache Exception Map: A map of return codes to error handlers.
This
only needs to be used if a return code
is being remapped on the site. So returning 404 for some 200 types.</li>
</ul>
<h4>Over-riding plugin settings</h4>
Most of the information in a plugin is fixed, but some items can be
over-ridden by the LOCKSS daemon's property settings, obtained locally
or from one of the LOCKSS property servers. The list of such
information is:<br>
<ul>
  <li>The refetch depth, which is the depth from the starting URL to
which the crawler's search for new content will proceed. If each new
article is linked from the publisher's manifest page directly,&nbsp;
this depth should be 1. If the manifest page links to the table of
content for an issue,&nbsp; which links to each new article this depth
should be 2, and so on. Each time it looks for new content, up to this
depth from the start URL, the crawler will re-fetch each URL even if it
is already in the cache.</li>
  <li>The new content crawl interval.</li>
  <li>A flag that enables and disables the crawl window (but not the
crawl window itself).</li>
  <li>The set of pre-configured titles.</li>
</ul>
<h4><br>
</h4>
<br>
</body>
</html>
