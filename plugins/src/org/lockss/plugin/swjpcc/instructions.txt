Go to www.swjpcc.com
Find and download all topic spreadsheets, volume spreadsheets.
There will be link duplication among them but to get a superset you need them all.
Note that they get added to so you need to download the latest copy of these plus any new ones that might have been added.

On Sep 19, 2018 these were:

Correspondence+11-29-16.xlsx
Critical+Care+8-16-18.xlsx
Critical+Care+Diagnosis+and+Gamuts+8-2-18.xlsx
Editorials+9-3-18.xlsx
General+Medicine+Postings+2-27-18.xls
General+Medicine+Postings+2-9-16.xls
Imaging+9-2-18.xlsx
Imaging+Diagnosis+and+Gamuts+8-6-18.xlsx
Medical+Image+of+the+Month+9-2-18.xlsx
News+6-16-18.xlsx
Proceedings+7-26-18.xlsx
Pulmonary+9-1-18.xlsx
Pulmonary+Diagnosis+and+Gamuts+9-1-18.xlsx
Sleep+2-5-18.xls
Ultrasounds+8-16-18.xls

Volume+1+9-7-11.xls
Volume+10.xls
Volume+11.xls
Volume+12.xls
Volume+13.xls
Volume+14.xlsx
Volume+15.xls
Volume+16.xlsx
Volume+2+12-27-11.xls
Volume+3.xlsx
Volume+4.xlsx
Volume+5.xls
Volume+6.xls
Volume+7.xls
Volume+8.xls
Volume+9.xls

Most+Recent+Volume+6-28-17.xlsx
Most+Recent+Volume+9-3-18.xlsx


Open each spreadsheet and save as Excel 2014 XML format.
Then pull out the links from the XML version of the spreadsheets in to one big sorted, uniq'd file.

egrep -o 'https?://[^"]+' *.xml | egrep -o 'https?://[^"]+' | sort -u > latest.urls

Then cat this list and the previous list to catch any that are no longer listed and to add in the http://www.swjpcc.com top url.
cat seed_urls.txt latest.urls | sort -u > <date>_seed_urls.txt

Check to pull out any not swjpcc base_host urls
egrep -o "^https?://[^/]+" <date>_seed_urls.txt | sort | uniq -c

Find and remove any  "w3.org" or any other urls that aren't on the base host or the two cdn hosts
https?://www.swjpcc.com
https?://static1.1.sqspcdn.com
https?://ereece.squarespace.com

REMOVE THE FOLLOWING 404 LINKS FROM the seed_urls list
These exist in the spreadsheets but no longer on the site.
The crawl won't fail because we allow 404 on start_urls, but it means the ServeContent will show a bunch of links that won't work so it's nicer not to have them as seeds.

http://www.swjpcc.com/editorial/ <--note singular

http://www.swjpcc.com/critical-care/2013/11/2/november-2013-pulmonary-case-of-the-month-a-series-of-unfort.html
http://www.swjpcc.com/editorials/2013/9/16/who-will-benefit-and-who-will-lose-from-obamacare.html
http://www.swjpcc.com/imaging/2013/6/5/image-of-the-week-crest-plus-ild.html
http://www.swjpcc.com/imaging/2015/6/10/medical-image-of-the-week-pulmonary-amyloidosis.html
http://www.swjpcc.com/imaging/2015/9/3/medical-image-of-the-week-fluorescent-urine.html
https://www.swjpcc.com/imaging/2021/4/30/may-2021-imaging-case-of-the-month-a-growing-indeterminate-s.html
https://www.swjpcc.com/pulmonary/2023/6/9/a-case-of-progressive-bleomycin-lung-toxicity-refractory-to.html
http://www.swjpcc.com/pulmonary-journal-club/* (there are 50 of these)
http://www.swjpcc.com/sleep-journal-club/* (there are 8 of these)



Now copy <date>_seed_urls.txt seed_urls.txt and check in. 
NOTE download date and final count of seed_urls in the check-in comment

Do a test crawl and remove any 404 urls from the seed list if that was their origin.

